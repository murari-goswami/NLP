{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP- word embeddings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXTpGKQqPuJp",
        "colab_type": "text"
      },
      "source": [
        "## Natural Language processing:- Word Embeddings\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*Y6dOZIKoJ1ZS_pzBzG6rgg.jpeg)\n",
        "\n",
        "\n",
        "\n",
        "##Word Vectors Introduction:-\n",
        "\n",
        "Word vectors represent a significant leap forward in advancing our ability to analyse relationships across words, sentences and documents. In doing so, they advance technology by providing machines much more information about words than has previously been possible using traditional representations of words. It is word vectors that make technologies such as speech recognition and machine translation possible. There are many excellent explanations of word vectors, but in this one I want to make the concept accessible to data and research people who aren’t very familiar with natural language processing (NLP).\n",
        "\n",
        "This blog post is divided into a few parts:-\n",
        "\n",
        "1) Word Vector and its implementation in spacy\n",
        "2) Word2vec and its Implementation in genism.\n",
        "3) Word2vec- The skip gram model and its implementation inTensorflow.\n",
        "4) GloVec and its implementation in genism.\n",
        "\n",
        "##What are Word Vectors ?\n",
        "\n",
        "\n",
        "Word vectors are simply vectors of numbers that represent the meaning of a word.\n",
        "\n",
        "1. Traditional approaches to NLP, such as one-hot encoding and bag-of-words models (i.e. using dummy variables to represent the presence or absence of a word in an observation (e.g. a sentence)), while it is useful for some machine learning (ML) tasks, do not capture information about a word’s meaning or context. \n",
        "2. This means that potential relationships, such as contextual closeness, are not captured across collections of words.\n",
        "3. For example, a one-hot encoding(Read my blog post [here](https://soumyadip1995.blogspot.com/2018/11/softmax-cross-entropy-and-logits.html))  cannot capture simple relationships, such as determining that the words “dog” and “cat” both refer to animals that are often discussed in the context of household pets. Such encodings often provide sufficient baselines for simple NLP tasks (for example, email spam classifiers), but lack the sophistication for more complex tasks such as translation and speech recognition.\n",
        "4. In essence, traditional approaches to NLP, such as one-hot encodings, do not capture syntactic (structure) and semantic (meaning) relationships across collections of words and, therefore, represent language in a very naive way.\n",
        "\n",
        "\n",
        "###So what does word Vectors represent ?\n",
        "\n",
        "1. In contrast, word vectors represent words as multidimensional continuous floating point numbers where semantically similar words are mapped to proximate points in geometric space. In simpler terms, a word vector is a row of real valued numbers (as opposed to dummy numbers) where each point captures a dimension of the word’s meaning and where semantically similar words have similar vectors.\n",
        "\n",
        "2. This means that words such as wheel and engine should have similar word vectors to the word car (because of the similarity of their meanings), whereas the word banana should be quite distant. Put differently, words that are used in a similar context will be mapped to a proximate vector space (we will get to how these word vectors are created below). \n",
        "\n",
        "####The beauty of representing words as vectors is that they lend themselves to mathematical operators. \n",
        "For example, we can add and subtract vectors — the example here is showing that by using word vectors we can determine that:\n",
        "\n",
        "**king — man + woman = queen**\n",
        "\n",
        "In other words, we can subtract one meaning from the word vector for king **(i.e. maleness)**, add another meaning **(femaleness)**, and show that this new word vector **(king — man + woman)** maps most closely to the word vector for queen.\n",
        "\n",
        "####The numbers in the word vector represent the word’s distributed weight across dimensions. In a simplified sense each dimension represents a meaning and the word’s numerical weight on that dimension captures the closeness of its association with and to that meaning. Thus, the semantics of the word are embedded across the dimensions of the vector.\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/0*mRGKYujQkI7PcMDE.)\n",
        "\n",
        "\n",
        "*In the figure we are imagining that each dimension captures a clearly defined meaning. For example, if you imagine that the first dimension represents the meaning or concept of “animal”, then each word’s weight on that dimension represents how closely it relates to that concept.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2myJFXfMTM_h",
        "colab_type": "text"
      },
      "source": [
        "###Lets Take a look at some code..!!\n",
        "\n",
        "Here we simply extract vectors for different animals and words that might be used to describe some of them\n",
        "\n",
        "1. !pip install spacy\n",
        "2. !pip install genism\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e__rgFvBTZlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "animals = \"dog cat hamster lion tiger elephant cheetah monkey gorilla antelope rabbit mouse rat zoo home pet fluffy wild domesticated\"\n",
        "\n",
        "animal_tokens = nlp(animals)\n",
        "\n",
        "animal_vectors = np.vstack([word.vector for word in animal_tokens if word.has_vector])\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "animal_vecs_transformed = pca.fit_transform(animal_vectors)\n",
        "animal_vecs_transformed = np.c_[animals.split(), animal_vecs_transformed]\n",
        "\n",
        "print(animal_vecs_transformed)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg1l9wCFltLC",
        "colab_type": "text"
      },
      "source": [
        "##Introduction to Word2Vec\n",
        "\n",
        "\n",
        "1. Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep nets can understand. Deeplearning4j implements a distributed form of Word2vec for Java and Scala, which works on Spark with GPUs.\n",
        "\n",
        "2. Word2vec’s applications extend beyond parsing sentences in the wild. It can be applied just as well to genes, code, likes, playlists, social media graphs and other verbal or symbolic series in which patterns may be discerned.\n",
        "\n",
        "3. Why? Because words are simply discrete states, and we are simply looking for the transitional probabilities between those states: the likelihood that they will co-occur\n",
        "\n",
        "4. The purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words.\n",
        "\n",
        "\n",
        "5. Given enough data, usage and contexts, Word2vec can make highly accurate guesses about a word’s meaning based on past appearances. Those guesses can be used to establish a word’s association with other words (e.g. “man” is to “boy” what “woman” is to “girl”), or cluster documents and classify them by topic. Those clusters can form the basis of search, sentiment analysis and recommendations in such diverse fields as scientific research, legal discovery, e-commerce and customer relationship management\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhYn3st9EuTk",
        "colab_type": "text"
      },
      "source": [
        "###Lets look at some more code..!!\n",
        "\n",
        "\n",
        "###There are two main training algorithms that can be used to learn the embedding from text; they are continuous bag of words (CBOW) and skip grams.\n",
        "\n",
        "![alt text](https://skymind.ai/images/wiki/word2vec_diagrams.png)\n",
        "\n",
        "Rather than loading a large text document or corpus from file, we will work with a small, in-memory list of pre-tokenized sentences. The model is trained and the minimum count for words is set to 1 so that no words are ignored.\n",
        "\n",
        "After the model is learned, we summarize, print the vocabulary, then print a single vector for the word ‘sentence‘.\n",
        "\n",
        "Finally, the model is saved to a file in binary format, loaded, and then summarized.\n",
        "\n",
        "##Visualize Word Embedding\n",
        "\n",
        "\n",
        "After you learn word embedding for your text data, it can be nice to explore it with visualization.\n",
        "\n",
        "You can use classical projection methods to reduce the high-dimensional word vectors to two-dimensional plots and plot them on a graph.\n",
        "\n",
        "The visualizations can provide a qualitative diagnostic for your learned model.\n",
        "\n",
        "We can retrieve all of the vectors from a trained model \n",
        "\n",
        "\n",
        "We can then train a projection method on the vectors, such as those methods offered in scikit-learn, then use matplotlib to plot the projection as a scatter plot.\n",
        "\n",
        "Let’s look at an example with Principal Component Analysis or PCA.\n",
        "\n",
        "##Plot Word Vectors Using PCA\n",
        "\n",
        "\n",
        "We can create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class.\n",
        "\n",
        "The resulting projection can be plotted using matplotlib as follows, pulling out the two dimensions as x and y coordinates.\n",
        "\n",
        "\n",
        "Putting this all together with the model from the previous section, the complete code is listed below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8wYSn8Hz8wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
        "\t\t\t['yet', 'another', 'sentence'],\n",
        "\t\t\t['one', 'more', 'sentence'],\n",
        "\t\t\t['and', 'the', 'final', 'sentence']]\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "# summarize the loaded model\n",
        "print(model)\n",
        "# summarize vocabulary\n",
        "words = list(model.wv.vocab)\n",
        "print(words)\n",
        "# access vector for one word\n",
        "print(model['sentence'])\n",
        "# save model\n",
        "model.save('model.bin')\n",
        "# load model\n",
        "new_model = Word2Vec.load('model.bin')\n",
        "print(new_model)\n",
        "\n",
        "X = model[model.wv.vocab]\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "\n",
        "words = list(model.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "  \n",
        "\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
        "\t\t\t['yet', 'another', 'sentence'],\n",
        "\t\t\t['one', 'more', 'sentence'],\n",
        "\t\t\t['and', 'the', 'final', 'sentence']]\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "# fit a 2d PCA model to the vectors\n",
        "X = model[model.wv.vocab]\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(model.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvma1R-DE0dl",
        "colab_type": "text"
      },
      "source": [
        "#### Running the example creates a scatter plot with the dots annotated with the words.It is hard to pull much meaning out of the graph given such a tiny corpus was used to fit the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nywXf8iyG_YS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Word2Vec (skip-gram model)\n",
        "\n",
        "\n",
        "The algorithm exists in two flavors CBOW and Skip-Gram. Given a set of sentences (also called corpus) the model loops on the words of each sentence and either tries to use the current word of to predict its neighbors (its context), in which case the method is called “Skip-Gram”, or it uses each of these contexts to predict the current word, in which case the method is called “Continuous Bag Of Words” (CBOW). The limit on the number of words in each context is determined by a parameter called “window size”.\n",
        "\n",
        "##Intuition\n",
        "\n",
        "The skip-gram neural network model is actually surprisingly simple in its most basic form. Train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.\n",
        "\n",
        "\n",
        "###As an example, let's consider the dataset\n",
        "\n",
        "**the quick brown fox jumped over the lazy dog**\n",
        "\n",
        "We first form a dataset of words and the contexts in which they appear. We could define 'context' in any way that makes sense, and in fact people have looked at syntactic contexts (i.e. the syntactic dependents of the current target word, see e.g. Levy et al.), words-to-the-left of the target, words-to-the-right of the target, etc. For now, let's stick to the vanilla definition and define 'context' as the window of words to the left and to the right of a target word. Using a window size of 1, we then have the dataset\n",
        "\n",
        "**([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox)**, ..\n",
        "\n",
        "of (context, target) pairs. Recall that skip-gram inverts contexts and targets, and tries to predict each context word from its target word, so the task becomes to predict 'the' and 'brown' from 'quick', 'quick' and 'fox' from 'brown', etc. Therefore our dataset becomes\n",
        "\n",
        "**(quick, the), (quick, brown), (brown, quick), (brown, fox)**, ...\n",
        "\n",
        "of **(input, output)** pairs. The objective function is defined over the entire dataset, but we typically optimize this with stochastic gradient descent (SGD) using one example at a time (or a 'minibatch' of batch_size examples, where typically 16 <= batch_size <= 512). \n",
        "\n",
        "###Visualization using tensorflow\n",
        "\n",
        "We can visualize the learned vectors by projecting them down to 2 dimensions using for instance something like the t-SNE dimensionality reduction technique[t-SNE](https://https://lvdmaaten.github.io/tsne/). When we inspect these visualizations it becomes apparent that the vectors capture some general, and in fact quite useful, semantic information about words and their relationships to one another. It was very interesting when we first discovered that certain directions in the induced vector space specialize towards certain semantic relationships, e.g. male-female, verb tense and even country-capital relationships between words, as illustrated in the figure below \n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9gfmreOgSw5",
        "colab_type": "text"
      },
      "source": [
        "##The Code\n",
        "\n",
        "###Visualizing the Learned Embeddings\n",
        "\n",
        "After training has finished we can visualize the learned embeddings using t-SNE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLlhSVND5y2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from tempfile import gettempdir\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "from six.moves import urllib\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "\n",
        "data_index = 0\n",
        "\n",
        "\n",
        "def word2vec_basic(log_dir):\n",
        "  \"\"\"Example of building, training and visualizing a word2vec model.\"\"\"\n",
        "  # Create the directory for TensorBoard variables if there is not.\n",
        "  if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "  # Step 1: Download the data.\n",
        "  url = 'http://mattmahoney.net/dc/'\n",
        "\n",
        "  # pylint: disable=redefined-outer-name\n",
        "  def maybe_download(filename, expected_bytes):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "    local_filename = os.path.join(gettempdir(), filename)\n",
        "    if not os.path.exists(local_filename):\n",
        "      local_filename, _ = urllib.request.urlretrieve(url + filename,\n",
        "                                                     local_filename)\n",
        "    statinfo = os.stat(local_filename)\n",
        "    if statinfo.st_size == expected_bytes:\n",
        "      print('Found and verified', filename)\n",
        "    else:\n",
        "      print(statinfo.st_size)\n",
        "      raise Exception('Failed to verify ' + local_filename +\n",
        "                      '. Can you get to it with a browser?')\n",
        "    return local_filename\n",
        "\n",
        "  filename = maybe_download('text8.zip', 31344016)\n",
        "\n",
        "  # Read the data into a list of strings.\n",
        "  def read_data(filename):\n",
        "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
        "    with zipfile.ZipFile(filename) as f:\n",
        "      data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "    return data\n",
        "\n",
        "  vocabulary = read_data(filename)\n",
        "  print('Data size', len(vocabulary))\n",
        "\n",
        "  # Step 2: Build the dictionary and replace rare words with UNK token.\n",
        "  vocabulary_size = 50000\n",
        "\n",
        "  def build_dataset(words, n_words):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    dictionary = {}\n",
        "    for word, _ in count:\n",
        "      dictionary[word] = len(dictionary)\n",
        "    data = []\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "      index = dictionary.get(word, 0)\n",
        "      if index == 0:  # dictionary['UNK']\n",
        "        unk_count += 1\n",
        "      data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reversed_dictionary\n",
        "\n",
        "  data, count, unused_dictionary, reverse_dictionary = build_dataset(\n",
        "      vocabulary, vocabulary_size)\n",
        "  del vocabulary  # Hint to reduce memory.\n",
        "  print('Most common words (+UNK)', count[:5])\n",
        "  print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
        "\n",
        "  # Step 3: Function to generate a training batch for the skip-gram model.\n",
        "  def generate_batch(batch_size, num_skips, skip_window):\n",
        "    global data_index\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * skip_window\n",
        "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
        "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
        "    if data_index + span > len(data):\n",
        "      data_index = 0\n",
        "    buffer.extend(data[data_index:data_index + span])\n",
        "    data_index += span\n",
        "    for i in range(batch_size // num_skips):\n",
        "      context_words = [w for w in range(span) if w != skip_window]\n",
        "      words_to_use = random.sample(context_words, num_skips)\n",
        "      for j, context_word in enumerate(words_to_use):\n",
        "        batch[i * num_skips + j] = buffer[skip_window]\n",
        "        labels[i * num_skips + j, 0] = buffer[context_word]\n",
        "      if data_index == len(data):\n",
        "        buffer.extend(data[0:span])\n",
        "        data_index = span\n",
        "      else:\n",
        "        buffer.append(data[data_index])\n",
        "        data_index += 1\n",
        "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
        "    data_index = (data_index + len(data) - span) % len(data)\n",
        "    return batch, labels\n",
        "\n",
        "  batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
        "  for i in range(8):\n",
        "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
        "          reverse_dictionary[labels[i, 0]])\n",
        "\n",
        "  # Step 4: Build and train a skip-gram model.\n",
        "\n",
        "\n",
        "  batch_size = 128\n",
        "  embedding_size = 128  # Dimension of the embedding vector.\n",
        "  skip_window = 1  # How many words to consider left and right.\n",
        "  num_skips = 2  # How many times to reuse an input to generate a label.\n",
        "  num_sampled = 64  # Number of negative examples to sample.\n",
        "\n",
        "  \n",
        "  valid_size = 16  # Random set of words to evaluate similarity on.\n",
        "  valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
        "  valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
        "\n",
        "  graph = tf.Graph()\n",
        "\n",
        "  with graph.as_default():\n",
        "\n",
        "  \n",
        "    # Input data.\n",
        "    with tf.name_scope('inputs'):\n",
        "      train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "      train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "      valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "\n",
        "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
        "    with tf.device('/cpu:0'):\n",
        "      # Look up embeddings for inputs.\n",
        "      with tf.name_scope('embeddings'):\n",
        "        embeddings = tf.Variable(\n",
        "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
        "\n",
        "      # Construct the variables for the NCE loss\n",
        "      with tf.name_scope('weights'):\n",
        "        nce_weights = tf.Variable(\n",
        "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
        "      with tf.name_scope('biases'):\n",
        "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "\n",
        "    \n",
        "    with tf.name_scope('loss'):\n",
        "      loss = tf.reduce_mean(\n",
        "          tf.nn.nce_loss(\n",
        "              weights=nce_weights,\n",
        "              biases=nce_biases,\n",
        "              labels=train_labels,\n",
        "              inputs=embed,\n",
        "              num_sampled=num_sampled,\n",
        "              num_classes=vocabulary_size))\n",
        "\n",
        "    # Add the loss value as a scalar to summary.\n",
        "    tf.summary.scalar('loss', loss)\n",
        "\n",
        "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
        "    with tf.name_scope('optimizer'):\n",
        "      optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
        "\n",
        "    # Compute the cosine similarity between minibatch examples and all\n",
        "    # embeddings.\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
        "    normalized_embeddings = embeddings / norm\n",
        "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
        "                                              valid_dataset)\n",
        "    similarity = tf.matmul(\n",
        "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
        "\n",
        "    # Merge all summaries.\n",
        "    merged = tf.summary.merge_all()\n",
        "\n",
        "    # Add variable initializer.\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Create a saver.\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "  # Step 5: Begin training.\n",
        "  num_steps = 100001\n",
        "\n",
        "  with tf.Session(graph=graph) as session:\n",
        "    # Open a writer to write summaries.\n",
        "    writer = tf.summary.FileWriter(log_dir, session.graph)\n",
        "\n",
        "    # We must initialize all variables before we use them.\n",
        "    init.run()\n",
        "    print('Initialized')\n",
        "\n",
        "    average_loss = 0\n",
        "    for step in xrange(num_steps):\n",
        "      batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
        "                                                  skip_window)\n",
        "      feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
        "\n",
        "      # Define metadata variable.\n",
        "      run_metadata = tf.RunMetadata()\n",
        "\n",
        "     \n",
        "      _, summary, loss_val = session.run([optimizer, merged, loss],\n",
        "                                         feed_dict=feed_dict,\n",
        "                                         run_metadata=run_metadata)\n",
        "      average_loss += loss_val\n",
        "\n",
        "      # Add returned summaries to writer in each step.\n",
        "      writer.add_summary(summary, step)\n",
        "      # Add metadata to visualize the graph for the last run.\n",
        "      if step == (num_steps - 1):\n",
        "        writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
        "\n",
        "      if step % 2000 == 0:\n",
        "        if step > 0:\n",
        "          average_loss /= 2000\n",
        "        # The average loss is an estimate of the loss over the last 2000\n",
        "        # batches.\n",
        "        print('Average loss at step ', step, ': ', average_loss)\n",
        "        average_loss = 0\n",
        "\n",
        "      # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "      if step % 10000 == 0:\n",
        "        sim = similarity.eval()\n",
        "        for i in xrange(valid_size):\n",
        "          valid_word = reverse_dictionary[valid_examples[i]]\n",
        "          top_k = 8  # number of nearest neighbors\n",
        "          nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
        "          log_str = 'Nearest to %s:' % valid_word\n",
        "          for k in xrange(top_k):\n",
        "            close_word = reverse_dictionary[nearest[k]]\n",
        "            log_str = '%s %s,' % (log_str, close_word)\n",
        "          print(log_str)\n",
        "    final_embeddings = normalized_embeddings.eval()\n",
        "\n",
        "    # Write corresponding labels for the embeddings.\n",
        "    with open(log_dir + '/metadata.tsv', 'w') as f:\n",
        "      for i in xrange(vocabulary_size):\n",
        "        f.write(reverse_dictionary[i] + '\\n')\n",
        "\n",
        "    # Save the model for checkpoints.\n",
        "    saver.save(session, os.path.join(log_dir, 'model.ckpt'))\n",
        "\n",
        "    # Create a configuration for visualizing embeddings with the labels in\n",
        "    # TensorBoard.\n",
        "    config = projector.ProjectorConfig()\n",
        "    embedding_conf = config.embeddings.add()\n",
        "    embedding_conf.tensor_name = embeddings.name\n",
        "    embedding_conf.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n",
        "    projector.visualize_embeddings(writer, config)\n",
        "\n",
        "  writer.close()\n",
        "\n",
        "  # Step 6: Visualize the embeddings.\n",
        "\n",
        "  # pylint: disable=missing-docstring\n",
        "  # Function to draw visualization of distance between embeddings.\n",
        "  def plot_with_labels(low_dim_embs, labels, filename):\n",
        "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
        "    plt.figure(figsize=(18, 18))  # in inches\n",
        "    for i, label in enumerate(labels):\n",
        "      x, y = low_dim_embs[i, :]\n",
        "      plt.scatter(x, y)\n",
        "      plt.annotate(\n",
        "          label,\n",
        "          xy=(x, y),\n",
        "          xytext=(5, 2),\n",
        "          textcoords='offset points',\n",
        "          ha='right',\n",
        "          va='bottom')\n",
        "\n",
        "    plt.savefig(filename)\n",
        "\n",
        "  try:\n",
        "    # pylint: disable=g-import-not-at-top\n",
        "    from sklearn.manifold import TSNE\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    tsne = TSNE(\n",
        "        perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
        "    plot_only = 500\n",
        "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
        "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
        "    plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(),\n",
        "                                                        'tsne.png'))\n",
        "\n",
        "  except ImportError as ex:\n",
        "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
        "    print(ex)\n",
        "\n",
        "\n",
        "# All functionality is run after tf.compat.v1.app.run() (b/122547914). This\n",
        "# could be split up but the methods are laid sequentially with their usage for\n",
        "# clarity.\n",
        "def main(unused_argv):\n",
        "  # Give a folder path as an argument with '--log_dir' to save\n",
        "  # TensorBoard summaries. Default is a log folder in current directory.\n",
        "  current_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\n",
        "      '--log_dir',\n",
        "      type=str,\n",
        "      default=os.path.join(current_path, 'log'),\n",
        "      help='The log directory for TensorBoard summaries.')\n",
        "  flags, unused_flags = parser.parse_known_args()\n",
        "  word2vec_basic(flags.log_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6hFLonHg1Wh",
        "colab_type": "text"
      },
      "source": [
        "##GloVe(Global Vectors)\n",
        "\n",
        "Another well-known model that learns vectors or words from their co-occurrence information, i.e. how frequently they appear together in large text corpora, is GlobalVectors (GloVe). While word2vec is a predictive model — a feed-forward neural network that learns vectors to improve the predictive ability, GloVe is a count-based model.\n",
        "\n",
        "##What is a count-based model?\n",
        "\n",
        "Generally speaking, count-based models learn vectors by doing dimensionality reduction on a co-occurrence counts matrix. First they construct a large matrix of co-occurrence information, which contains the information on how frequently each “word” (stored in rows), is seen in some “context” (the columns). The number of “contexts” needs be large, since it is essentially combinatorial in size. Afterwards they factorize this matrix to yield a lower-dimensional matrix of words and features, where each row yields a vector representation for each word. It is achieved by minimizing a “reconstruction loss” which looks for lower-dimensional representations that can explain the variance in the high-dimensional data.\n",
        "\n",
        "In the case of GloVe, the counts matrix is preprocessed by normalizing the counts and log-smoothing them. Compared to word2vec, GloVe allows for parallel implementation, which means that it’s easier to train over more data. It is believed (GloVe) to combine the benefits of the word2vec skip-gram model in the word analogy tasks, with those of matrix factorization methods exploiting global statistical information.\n",
        "\n",
        "##GloVe at a Glance\n",
        "\n",
        " GloVe is essentially a log-bilinear model with a weighted least-squares objective. The model rests on a rather simple idea that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning which can be encoded as vector differences. Therefore, the training objective is to learn word vectors such that their dot product equals the logarithm of the words’ probability of co-occurrence. As the logarithm of a ratio equals the difference of logarithms, this objective associates the ratios of co-occurrence probabilities with vector differences in the word vector space. It creates the word vectors that perform well on both word analogy tasks and on similarity tasks and named entity recognition\n",
        " \n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AROXuRsH5zsk",
        "colab_type": "text"
      },
      "source": [
        "##Load Stanford’s GloVe Embedding\n",
        "Stanford researchers also have their own word embedding algorithm like word2vec called Global Vectors for Word Representation, or GloVe for short.\n",
        "\n",
        "I won’t get into the details of the differences between word2vec and GloVe here, but generally, NLP practitioners seem to prefer GloVe at the moment based on results.\n",
        "\n",
        "Like word2vec, the GloVe researchers also provide pre-trained word vectors, in this case, a great selection to choose from.\n",
        "\n",
        "You can download the GloVe pre-trained word vectors and load them easily with gensim.\n",
        "\n",
        "The first step is to convert the GloVe file format to the word2vec file format. The only difference is the addition of a small header line. This can be done by calling the *glove2word2vec() *function. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhFAd-6d42ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file = 'glove.txt'\n",
        "word2vec_output_file = 'word2vec.txt'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCnJG5QL6CwV",
        "colab_type": "text"
      },
      "source": [
        "Once converted, the file can be loaded just like word2vec file above.\n",
        "\n",
        "Let’s make this concrete with an example.\n",
        "\n",
        "You can download the smallest GloVe pre-trained model from the GloVe website. It an 822 Megabyte zip file with 4 different models (50, 100, 200 and 300-dimensional vectors) trained on Wikipedia data with 6 billion tokens and a 400,000 word vocabulary.\n",
        "\n",
        "The direct download link is here:\n",
        "[glove.6B.zip](https://http://nlp.stanford.edu/data/glove.6B.zip)\n",
        "\n",
        "Working with the 100-dimensional version of the model, we can convert the file to word2vec format as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BFX61AA6Sxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file = 'glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxIyixIZ6dvF",
        "colab_type": "text"
      },
      "source": [
        "You now have a copy of the GloVe model in word2vec format with the filename glove.6B.100d.txt.word2vec.\n",
        "\n",
        "Now we can load it and perform the same (king – man) + woman = ? test(as in the instagram post) The complete code listing is provided below. Note that the converted file is ASCII format, not binary, so we set binary=False when loading.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJvDuZW_6uOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "# load the Stanford GloVe model\n",
        "filename = 'glove.6B.100d.txt.word2vec'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
        "# calculate: (king - man) + woman = ?\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yG0TbEF6xlf",
        "colab_type": "text"
      },
      "source": [
        "Running the example prints the same result of ‘queen’.\n",
        "\n",
        "Result-> [('queen', 0.7698540687561035)]\n"
      ]
    }
  ]
}